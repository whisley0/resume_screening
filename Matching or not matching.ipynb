{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ec34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import PyPDF2\n",
    "import textract\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import docx2txt\n",
    "\n",
    "resume = 'Chong Fai Edward Lam Resume.docx'\n",
    "\n",
    "job_description = 'job description - pt form.docx'\n",
    "\n",
    "def read_input_file(filename):\n",
    "\n",
    "    if 'pdf' in filename:\n",
    "        # Open pdf file\n",
    "        pdfFileObj = open('sample_resume_.pdf','rb')\n",
    "\n",
    "        # Read file\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "        # Get total number of pages\n",
    "        num_pages = pdfReader.numPages\n",
    "\n",
    "        # Initialize a count for the number of pages\n",
    "        count = 0\n",
    "\n",
    "        # Initialize a text empty etring variable\n",
    "        text = \"\"\n",
    "\n",
    "        # Extract text from every page on the file\n",
    "        while count < num_pages:\n",
    "            pageObj = pdfReader.getPage(count)\n",
    "            count +=1\n",
    "            text += pageObj.extractText()\n",
    "        \n",
    "    elif 'doc' in filename:\n",
    "\n",
    "        # Passing docx file to process function\n",
    "        text = docx2txt.process(filename)\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63e2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleansing(text):\n",
    "    # Convert all strings to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+','',text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    return text.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "807482c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_txt = text_cleansing(read_input_file(resume))\n",
    "jd_txt = text_cleansing(read_input_file(job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4addf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'database related skills \\n\\nmysql \\n\\nms server sql\\n\\npostgresql\\n\\nweb application \\n\\nnodejs \\n\\ndjango \\n\\nhtml \\n\\ncss \\n\\njavascript \\n\\najax \\n\\nphp \\n\\nreactjs\\n\\nscraping experience\\n\\naws related services\\n\\ns\\n\\nlambda\\n\\nec\\n\\nrds\\n\\nglue\\n\\nredshift\\n\\nairflow\\n\\nvisualization skills\\n\\ntableau\\n\\npowerbi\\n\\nsoftware development in python\\n\\nproficiency in data warehouse concepts and data engineer related activities such as data mapping data cleansing and ingestion to data warehouse datamart generation using stored procedure etc\\n\\ndata mapping\\n\\ndatamart\\n\\ndata warehouse\\n\\ncomfortable in working in agile development cycle keeping track of jira tickets contacting business for requirement confirmation overseeing the development and contributing in scrum meetings\\n\\nagile\\n\\njira\\n\\nscrum'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jd_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that does phrase matching and builds a candidate profile\n",
    "def create_profile(file):\n",
    "    text = pdfextract(file) \n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.lower()\n",
    "    #below is the csv where we have all the keywords, you can customize your own\n",
    "    keyword_dict = pd.read_csv('D:/NLP_Resume/resume/template_new.csv')\n",
    "    stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
    "    NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
    "    ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "    DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
    "    R_words = [nlp(text) for text in keyword_dict['R Language'].dropna(axis = 0)]\n",
    "    python_words = [nlp(text) for text in keyword_dict['Python Language'].dropna(axis = 0)]\n",
    "    Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]\n",
    "\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add('Stats', None, *stats_words)\n",
    "    matcher.add('NLP', None, *NLP_words)\n",
    "    matcher.add('ML', None, *ML_words)\n",
    "    matcher.add('DL', None, *DL_words)\n",
    "    matcher.add('R', None, *R_words)\n",
    "    matcher.add('Python', None, *python_words)\n",
    "    matcher.add('DE', None, *Data_Engineering_words)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    d = []  \n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "        span = doc[start : end]  # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))      \n",
    "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())\n",
    "    \n",
    "    ## convertimg string of keywords to dataframe\n",
    "    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
    "    df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
    "    df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
    "    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
    "    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
    "    \n",
    "    base = os.path.basename(file)\n",
    "    filename = os.path.splitext(base)[0]\n",
    "       \n",
    "    name = filename.split('_')\n",
    "    name2 = name[0]\n",
    "    name2 = name2.lower()\n",
    "    ## converting str to dataframe\n",
    "    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
    "    \n",
    "    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
    "    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
    "\n",
    "    return(dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86b12b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_txt_list = jd_txt.split(\"\\n\")\n",
    "jd_txt_list = list(dict.fromkeys(jd_txt_list))\n",
    "\n",
    "matching_word = []\n",
    "not_matching_word = []\n",
    "\n",
    "for word in jd_txt_list:\n",
    "    if word in resume_txt:\n",
    "        matching_word.append(word)\n",
    "    else:\n",
    "        not_matching_word.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81f01f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 's',\n",
       " 'lambda',\n",
       " 'ec',\n",
       " 'glue',\n",
       " 'redshift',\n",
       " 'airflow',\n",
       " 'tableau',\n",
       " 'data mapping',\n",
       " 'data warehouse',\n",
       " 'scrum']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6d3130f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['database related skills ',\n",
       " 'mysql ',\n",
       " 'ms server sql',\n",
       " 'postgresql',\n",
       " 'web application ',\n",
       " 'nodejs ',\n",
       " 'django ',\n",
       " 'html ',\n",
       " 'css ',\n",
       " 'javascript ',\n",
       " 'ajax ',\n",
       " 'php ',\n",
       " 'reactjs',\n",
       " 'scraping experience',\n",
       " 'aws related services',\n",
       " 'rds',\n",
       " 'visualization skills',\n",
       " 'powerbi',\n",
       " 'software development in python',\n",
       " 'proficiency in data warehouse concepts and data engineer related activities such as data mapping data cleansing and ingestion to data warehouse datamart generation using stored procedure etc',\n",
       " 'datamart',\n",
       " 'comfortable in working in agile development cycle keeping track of jira tickets contacting business for requirement confirmation overseeing the development and contributing in scrum meetings',\n",
       " 'agile',\n",
       " 'jira']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_matching_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369028f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
